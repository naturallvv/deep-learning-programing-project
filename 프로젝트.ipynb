{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b9a397e1-1aa1-49d4-952d-571ef62986a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import *\n",
    "from sklearn.preprocessing import *\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d1d04ecb-8f9b-41e6-8918-cc39c8d447a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f9e38f36-b341-4e05-87c8-96681b8d80e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LABEL</th>\n",
       "      <th>FLUX.1</th>\n",
       "      <th>FLUX.2</th>\n",
       "      <th>FLUX.3</th>\n",
       "      <th>FLUX.4</th>\n",
       "      <th>FLUX.5</th>\n",
       "      <th>FLUX.6</th>\n",
       "      <th>FLUX.7</th>\n",
       "      <th>FLUX.8</th>\n",
       "      <th>FLUX.9</th>\n",
       "      <th>...</th>\n",
       "      <th>FLUX.3188</th>\n",
       "      <th>FLUX.3189</th>\n",
       "      <th>FLUX.3190</th>\n",
       "      <th>FLUX.3191</th>\n",
       "      <th>FLUX.3192</th>\n",
       "      <th>FLUX.3193</th>\n",
       "      <th>FLUX.3194</th>\n",
       "      <th>FLUX.3195</th>\n",
       "      <th>FLUX.3196</th>\n",
       "      <th>FLUX.3197</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>93.85</td>\n",
       "      <td>83.81</td>\n",
       "      <td>20.10</td>\n",
       "      <td>-26.98</td>\n",
       "      <td>-39.56</td>\n",
       "      <td>-124.71</td>\n",
       "      <td>-135.18</td>\n",
       "      <td>-96.27</td>\n",
       "      <td>-79.89</td>\n",
       "      <td>...</td>\n",
       "      <td>-78.07</td>\n",
       "      <td>-102.15</td>\n",
       "      <td>-102.15</td>\n",
       "      <td>25.13</td>\n",
       "      <td>48.57</td>\n",
       "      <td>92.54</td>\n",
       "      <td>39.32</td>\n",
       "      <td>61.42</td>\n",
       "      <td>5.08</td>\n",
       "      <td>-39.54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>-38.88</td>\n",
       "      <td>-33.83</td>\n",
       "      <td>-58.54</td>\n",
       "      <td>-40.09</td>\n",
       "      <td>-79.31</td>\n",
       "      <td>-72.81</td>\n",
       "      <td>-86.55</td>\n",
       "      <td>-85.33</td>\n",
       "      <td>-83.97</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.28</td>\n",
       "      <td>-32.21</td>\n",
       "      <td>-32.21</td>\n",
       "      <td>-24.89</td>\n",
       "      <td>-4.86</td>\n",
       "      <td>0.76</td>\n",
       "      <td>-11.70</td>\n",
       "      <td>6.46</td>\n",
       "      <td>16.00</td>\n",
       "      <td>19.93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>532.64</td>\n",
       "      <td>535.92</td>\n",
       "      <td>513.73</td>\n",
       "      <td>496.92</td>\n",
       "      <td>456.45</td>\n",
       "      <td>466.00</td>\n",
       "      <td>464.50</td>\n",
       "      <td>486.39</td>\n",
       "      <td>436.56</td>\n",
       "      <td>...</td>\n",
       "      <td>-71.69</td>\n",
       "      <td>13.31</td>\n",
       "      <td>13.31</td>\n",
       "      <td>-29.89</td>\n",
       "      <td>-20.88</td>\n",
       "      <td>5.06</td>\n",
       "      <td>-11.80</td>\n",
       "      <td>-28.91</td>\n",
       "      <td>-70.02</td>\n",
       "      <td>-96.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>326.52</td>\n",
       "      <td>347.39</td>\n",
       "      <td>302.35</td>\n",
       "      <td>298.13</td>\n",
       "      <td>317.74</td>\n",
       "      <td>312.70</td>\n",
       "      <td>322.33</td>\n",
       "      <td>311.31</td>\n",
       "      <td>312.42</td>\n",
       "      <td>...</td>\n",
       "      <td>5.71</td>\n",
       "      <td>-3.73</td>\n",
       "      <td>-3.73</td>\n",
       "      <td>30.05</td>\n",
       "      <td>20.03</td>\n",
       "      <td>-12.67</td>\n",
       "      <td>-8.77</td>\n",
       "      <td>-17.31</td>\n",
       "      <td>-17.35</td>\n",
       "      <td>13.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>-1107.21</td>\n",
       "      <td>-1112.59</td>\n",
       "      <td>-1118.95</td>\n",
       "      <td>-1095.10</td>\n",
       "      <td>-1057.55</td>\n",
       "      <td>-1034.48</td>\n",
       "      <td>-998.34</td>\n",
       "      <td>-1022.71</td>\n",
       "      <td>-989.57</td>\n",
       "      <td>...</td>\n",
       "      <td>-594.37</td>\n",
       "      <td>-401.66</td>\n",
       "      <td>-401.66</td>\n",
       "      <td>-357.24</td>\n",
       "      <td>-443.76</td>\n",
       "      <td>-438.54</td>\n",
       "      <td>-399.71</td>\n",
       "      <td>-384.65</td>\n",
       "      <td>-411.79</td>\n",
       "      <td>-510.54</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 3198 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   LABEL   FLUX.1   FLUX.2   FLUX.3   FLUX.4   FLUX.5   FLUX.6  FLUX.7  \\\n",
       "0      2    93.85    83.81    20.10   -26.98   -39.56  -124.71 -135.18   \n",
       "1      2   -38.88   -33.83   -58.54   -40.09   -79.31   -72.81  -86.55   \n",
       "2      2   532.64   535.92   513.73   496.92   456.45   466.00  464.50   \n",
       "3      2   326.52   347.39   302.35   298.13   317.74   312.70  322.33   \n",
       "4      2 -1107.21 -1112.59 -1118.95 -1095.10 -1057.55 -1034.48 -998.34   \n",
       "\n",
       "    FLUX.8  FLUX.9  ...  FLUX.3188  FLUX.3189  FLUX.3190  FLUX.3191  \\\n",
       "0   -96.27  -79.89  ...     -78.07    -102.15    -102.15      25.13   \n",
       "1   -85.33  -83.97  ...      -3.28     -32.21     -32.21     -24.89   \n",
       "2   486.39  436.56  ...     -71.69      13.31      13.31     -29.89   \n",
       "3   311.31  312.42  ...       5.71      -3.73      -3.73      30.05   \n",
       "4 -1022.71 -989.57  ...    -594.37    -401.66    -401.66    -357.24   \n",
       "\n",
       "   FLUX.3192  FLUX.3193  FLUX.3194  FLUX.3195  FLUX.3196  FLUX.3197  \n",
       "0      48.57      92.54      39.32      61.42       5.08     -39.54  \n",
       "1      -4.86       0.76     -11.70       6.46      16.00      19.93  \n",
       "2     -20.88       5.06     -11.80     -28.91     -70.02     -96.67  \n",
       "3      20.03     -12.67      -8.77     -17.31     -17.35      13.98  \n",
       "4    -443.76    -438.54    -399.71    -384.65    -411.79    -510.54  \n",
       "\n",
       "[5 rows x 3198 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2a7f3e0b-49e0-4388-b8cc-f3be051a89cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LABEL        0\n",
       "FLUX.1       0\n",
       "FLUX.2       0\n",
       "FLUX.3       0\n",
       "FLUX.4       0\n",
       "            ..\n",
       "FLUX.3193    0\n",
       "FLUX.3194    0\n",
       "FLUX.3195    0\n",
       "FLUX.3196    0\n",
       "FLUX.3197    0\n",
       "Length: 3198, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c1bca06f-6a72-4422-bf5a-eda27396675d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_data.drop(columns=['LABEL'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "192cf20d-d030-45cf-adc8-59f8a02aaafe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-39.54,  19.93, -96.67, ...,  79.43,  -2.55,  27.82])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = train_data.iloc[:, :-1].values  # FLUX1부터 FLUX3196까지\n",
    "y = train_data.iloc[:, -1].values   # FLUX3197\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "91830468-90a4-4f20-b569-c293c6a64b9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((5087, 3196), (5087, 1))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler_X = StandardScaler()\n",
    "scaler_y = StandardScaler()\n",
    "X_scaled = scaler_X.fit_transform(X)\n",
    "y_scaled = scaler_y.fit_transform(y.reshape(-1, 1))\n",
    "\n",
    "X_scaled.shape,y_scaled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eb7ec89d-eb03-4ec7-b331-0cc91607b26f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sequences(data, labels, seq_length):\n",
    "    sequences = []\n",
    "    seq_labels = []\n",
    "    for i in range(len(data) - seq_length):\n",
    "        sequences.append(data[i:i+seq_length])\n",
    "        seq_labels.append(labels[i+seq_length])\n",
    "    return np.array(sequences), np.array(seq_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "860712ea-b402-4773-8cf0-68c6996b403f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 시퀀스 길이 설정\n",
    "seq_length = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f2bf9156-968c-472a-b027-42668a8129b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 시퀀스 데이터 생성\n",
    "X_sequences, y_sequences = create_sequences(X_scaled, y_scaled, seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4cb13ba5-3976-4071-9376-87abdf633787",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5062, 1)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_sequences.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "539f8697-14d4-42c8-9c9a-f8d3efb2b8ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X_sequences, y_sequences, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e8ff34b6-a73d-4ae3-9be1-390db3dc5089",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 텐서로 변환 및 GPU로 이동\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32).to(device)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.float32).to(device)\n",
    "X_val_tensor = torch.tensor(X_val, dtype=torch.float32).to(device)\n",
    "y_val_tensor = torch.tensor(y_val, dtype=torch.float32).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d3334937-4626-4491-85dc-f9ce8e915c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, num_classes, input_size, hidden_size, num_layers, seq_length):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.num_layers = num_layers\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.seq_length = seq_length\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size=input_size, hidden_size=hidden_size,\n",
    "                            num_layers=num_layers, batch_first=True)\n",
    "        self.fc_1 = nn.Linear(hidden_size, 128)\n",
    "        self.fc = nn.Linear(128, num_classes)\n",
    "        self.dropout = nn.Dropout(0.3)  # 드롭아웃 비율 조정\n",
    "        self.batch_norm = nn.BatchNorm1d(128)  # 배치 정규화 레이어 추가\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        h_0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n",
    "        c_0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n",
    "        \n",
    "        output, (hn, cn) = self.lstm(x, (h_0, c_0))\n",
    "        hn = hn.view(-1, self.hidden_size)\n",
    "        out = self.relu(hn)\n",
    "        out = self.fc_1(out)\n",
    "        out = self.batch_norm(out)  # 배치 정규화 적용\n",
    "        out = self.relu(out)\n",
    "        out = self.dropout(out)  # 드롭아웃 적용\n",
    "        out = self.fc(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "acbbd1cb-92f2-43dc-ab6a-47aef20e2408",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 하이퍼파라미터 설정\n",
    "input_dim = X_train_tensor.shape[2]\n",
    "hidden_dim = 64\n",
    "layer_dim = 1\n",
    "output_dim = X_train_tensor.shape[2]  # 예측할 값의 차원"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "84332ff6-d904-4610-a342-e24b14e13a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 초기화 및 GPU로 이동\n",
    "model = LSTM(output_dim, input_dim, hidden_dim, layer_dim, seq_length).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "168a71b6-ac99-4f1c-ac72-80054c225913",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 손실 함수 및 최적화 알고리즘 설정\n",
    "criterion = nn.MSELoss().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2c2e3f94-41cc-4cfa-ac08-7838fa3bd3e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([4049, 1])) that is different to the input size (torch.Size([4049, 3196])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "C:\\Users\\user\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([1013, 1])) that is different to the input size (torch.Size([1013, 3196])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0/1000], Loss: 0.9265, Val Loss: 2.2711\n",
      "Epoch [10/1000], Loss: 0.7467, Val Loss: 2.2701\n",
      "Epoch [20/1000], Loss: 0.7073, Val Loss: 2.2696\n",
      "Epoch [30/1000], Loss: 0.6921, Val Loss: 2.2694\n",
      "Epoch [40/1000], Loss: 0.6668, Val Loss: 2.2709\n",
      "Epoch [50/1000], Loss: 0.5669, Val Loss: 2.2740\n",
      "Epoch [60/1000], Loss: 0.3391, Val Loss: 2.2751\n",
      "Epoch [70/1000], Loss: 0.2733, Val Loss: 2.2836\n",
      "Epoch [80/1000], Loss: 0.2469, Val Loss: 2.3250\n",
      "Epoch [90/1000], Loss: 0.2319, Val Loss: 2.3675\n",
      "Epoch [100/1000], Loss: 0.2278, Val Loss: 2.3217\n",
      "Epoch [110/1000], Loss: 0.2158, Val Loss: 2.4020\n",
      "Epoch [120/1000], Loss: 0.2345, Val Loss: 2.3686\n",
      "Epoch [130/1000], Loss: 0.2069, Val Loss: 2.3635\n",
      "Epoch [140/1000], Loss: 0.1654, Val Loss: 2.3705\n",
      "Epoch [150/1000], Loss: 0.1821, Val Loss: 2.3801\n",
      "Epoch [160/1000], Loss: 0.1188, Val Loss: 2.3305\n",
      "Epoch [170/1000], Loss: 0.0984, Val Loss: 2.3759\n",
      "Epoch [180/1000], Loss: 0.1030, Val Loss: 2.5034\n",
      "Epoch [190/1000], Loss: 0.1003, Val Loss: 2.3573\n",
      "Epoch [200/1000], Loss: 0.1080, Val Loss: 2.5721\n",
      "Epoch [210/1000], Loss: 0.0929, Val Loss: 2.6175\n",
      "Epoch [220/1000], Loss: 0.0684, Val Loss: 2.6286\n",
      "Epoch [230/1000], Loss: 0.0565, Val Loss: 2.5036\n",
      "Epoch [240/1000], Loss: 0.0531, Val Loss: 2.4736\n",
      "Epoch [250/1000], Loss: 0.0497, Val Loss: 2.6397\n",
      "Epoch [260/1000], Loss: 0.0340, Val Loss: 2.7300\n",
      "Epoch [270/1000], Loss: 0.0347, Val Loss: 2.8725\n",
      "Epoch [280/1000], Loss: 0.0313, Val Loss: 2.8896\n",
      "Epoch [290/1000], Loss: 0.0372, Val Loss: 3.0694\n",
      "Epoch [300/1000], Loss: 0.0384, Val Loss: 2.9357\n",
      "Epoch [310/1000], Loss: 0.0296, Val Loss: 2.8245\n",
      "Epoch [320/1000], Loss: 0.0254, Val Loss: 3.0988\n",
      "Epoch [330/1000], Loss: 0.0206, Val Loss: 2.9091\n",
      "Epoch [340/1000], Loss: 0.0233, Val Loss: 3.0258\n",
      "Epoch [350/1000], Loss: 0.0170, Val Loss: 2.9084\n",
      "Epoch [360/1000], Loss: 0.0146, Val Loss: 2.9068\n",
      "Epoch [370/1000], Loss: 0.0214, Val Loss: 2.8982\n",
      "Epoch [380/1000], Loss: 0.0201, Val Loss: 2.8705\n",
      "Epoch [390/1000], Loss: 0.0285, Val Loss: 2.9888\n",
      "Epoch [400/1000], Loss: 0.0248, Val Loss: 3.0142\n",
      "Epoch [410/1000], Loss: 0.0395, Val Loss: 2.9201\n",
      "Epoch [420/1000], Loss: 0.0203, Val Loss: 3.1177\n",
      "Epoch [430/1000], Loss: 0.0200, Val Loss: 3.0856\n",
      "Epoch [440/1000], Loss: 0.0142, Val Loss: 3.1212\n",
      "Epoch [450/1000], Loss: 0.0139, Val Loss: 3.0076\n",
      "Epoch [460/1000], Loss: 0.0130, Val Loss: 3.0830\n",
      "Epoch [470/1000], Loss: 0.0104, Val Loss: 2.9648\n",
      "Epoch [480/1000], Loss: 0.0165, Val Loss: 2.9160\n",
      "Epoch [490/1000], Loss: 0.0089, Val Loss: 2.9423\n",
      "Epoch [500/1000], Loss: 0.0145, Val Loss: 2.9903\n",
      "Epoch [510/1000], Loss: 0.0190, Val Loss: 2.9022\n",
      "Epoch [520/1000], Loss: 0.0131, Val Loss: 3.0614\n",
      "Epoch [530/1000], Loss: 0.0124, Val Loss: 2.8654\n",
      "Epoch [540/1000], Loss: 0.0130, Val Loss: 2.9203\n",
      "Epoch [550/1000], Loss: 0.0121, Val Loss: 2.8872\n",
      "Epoch [560/1000], Loss: 0.0206, Val Loss: 2.9251\n",
      "Epoch [570/1000], Loss: 0.0104, Val Loss: 2.8990\n",
      "Epoch [580/1000], Loss: 0.0161, Val Loss: 2.8681\n",
      "Epoch [590/1000], Loss: 0.0239, Val Loss: 2.9595\n",
      "Epoch [600/1000], Loss: 0.0141, Val Loss: 2.9306\n",
      "Epoch [610/1000], Loss: 0.0127, Val Loss: 2.8499\n",
      "Epoch [620/1000], Loss: 0.0232, Val Loss: 2.7742\n",
      "Epoch [630/1000], Loss: 0.0103, Val Loss: 2.8426\n",
      "Epoch [640/1000], Loss: 0.0089, Val Loss: 2.8748\n",
      "Epoch [650/1000], Loss: 0.0087, Val Loss: 2.8156\n",
      "Epoch [660/1000], Loss: 0.0141, Val Loss: 2.8325\n",
      "Epoch [670/1000], Loss: 0.0226, Val Loss: 2.9190\n",
      "Epoch [680/1000], Loss: 0.0149, Val Loss: 2.8418\n",
      "Epoch [690/1000], Loss: 0.0094, Val Loss: 2.7281\n",
      "Epoch [700/1000], Loss: 0.0127, Val Loss: 2.7927\n",
      "Epoch [710/1000], Loss: 0.0104, Val Loss: 2.6654\n",
      "Epoch [720/1000], Loss: 0.0052, Val Loss: 2.7163\n",
      "Epoch [730/1000], Loss: 0.0200, Val Loss: 2.9409\n",
      "Epoch [740/1000], Loss: 0.0145, Val Loss: 2.7340\n",
      "Epoch [750/1000], Loss: 0.0131, Val Loss: 2.7472\n",
      "Epoch [760/1000], Loss: 0.0088, Val Loss: 2.6850\n",
      "Epoch [770/1000], Loss: 0.0206, Val Loss: 2.6644\n",
      "Epoch [780/1000], Loss: 0.0059, Val Loss: 2.6577\n",
      "Epoch [790/1000], Loss: 0.0082, Val Loss: 2.6626\n",
      "Epoch [800/1000], Loss: 0.0095, Val Loss: 2.8855\n",
      "Epoch [810/1000], Loss: 0.0152, Val Loss: 2.6010\n",
      "Epoch [820/1000], Loss: 0.0135, Val Loss: 2.9187\n",
      "Epoch [830/1000], Loss: 0.0090, Val Loss: 2.5970\n",
      "Epoch [840/1000], Loss: 0.0180, Val Loss: 2.6465\n",
      "Epoch [850/1000], Loss: 0.0075, Val Loss: 2.7177\n",
      "Epoch [860/1000], Loss: 0.0105, Val Loss: 2.6613\n",
      "Epoch [870/1000], Loss: 0.0141, Val Loss: 2.6431\n",
      "Epoch [880/1000], Loss: 0.0083, Val Loss: 2.6120\n",
      "Epoch [890/1000], Loss: 0.0126, Val Loss: 2.5797\n",
      "Epoch [900/1000], Loss: 0.0208, Val Loss: 2.7256\n",
      "Epoch [910/1000], Loss: 0.0085, Val Loss: 2.6972\n",
      "Epoch [920/1000], Loss: 0.0055, Val Loss: 2.6462\n",
      "Epoch [930/1000], Loss: 0.0125, Val Loss: 2.6762\n",
      "Epoch [940/1000], Loss: 0.0246, Val Loss: 2.5655\n",
      "Epoch [950/1000], Loss: 0.0089, Val Loss: 2.6771\n",
      "Epoch [960/1000], Loss: 0.0072, Val Loss: 2.5995\n",
      "Epoch [970/1000], Loss: 0.0056, Val Loss: 2.7168\n",
      "Epoch [980/1000], Loss: 0.0156, Val Loss: 2.5469\n",
      "Epoch [990/1000], Loss: 0.0118, Val Loss: 2.6789\n"
     ]
    }
   ],
   "source": [
    "# 모델 학습\n",
    "num_epochs = 1000\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(X_train_tensor)\n",
    "    loss = criterion(outputs, y_train_tensor)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # 검증 데이터에 대한 성능 평가\n",
    "    if epoch % 10 == 0:\n",
    "        model.eval()\n",
    "        val_outputs = model(X_val_tensor)\n",
    "        val_loss = criterion(val_outputs, y_val_tensor)\n",
    "        print(f'Epoch [{epoch}/{num_epochs}], Loss: {loss.item():.4f}, Val Loss: {val_loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8cf332f-d3e7-4ca0-bea1-e0746e1bbda6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
